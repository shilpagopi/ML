# Positional Encodings

## Sinusoidal Positional Encoding
The core idea is to represent each position in a binary-like fashion, where each dimension of the positional encoding vector corresponds to a specific frequency.

positional embedding dimension = word embeddings dimension (bcoz they are added element-wise)
<img width="750" height="500" alt="image" src="https://github.com/user-attachments/assets/b1eb7193-ae44-4335-8b89-c0be12b814ee" />

The self-attention mechanism in the Transformer, by its very nature, is **permutation invariant**. This means if you were to randomly shuffle the words in a sentence, the self-attention mechanism would produce the exact same output, as it treats the input as a "bag of words" without any regard for their original order. Without Positional Encodings, the model would be unable to differentiate between sentences like "The dog chased the cat" and "The cat chased the dog".

## Benefits: 
* **Relative Position Information**: A crucial property is that for any fixed offset k, the positional encoding for PE(pos + k) can be represented as a linear function of PE(pos).
* **Uniqueness**: Because the frequencies are different for each dimension, every position in the sequence gets a unique encoding vector. This allows the model to differentiate between any two positions
* **Bounded Values**: The sine and cosine functions always produce values between -1 and 1. This keeps the positional encoding values within a normalized range, preventing them from destabilizing the training process as sequence lengths increase.
* **Extrapolation**: Since the positional encodings are generated by a deterministic function rather than being learned, the model can extrapolate to sequences longer than those seen during training. It can simply compute the positional encodings for a new, longer sequence on the fly. This is a major advantage over learned positional embeddings, which are limited to the maximum sequence length seen during training.

## Alternatives:
* Learnable Positional Embeddings: Instead of using a fixed function, a separate embedding matrix is created for positions, and these position vectors are learned during training. Models like BERT use this approach. A key advantage is that the model can learn more flexible and task-specific position representations. A disadvantage is that the model cannot extrapolate to sequence lengths longer than those seen during training, as it has not learned a positional embedding for those new positions.

* Relative Positional Encodings: These methods, used in models like Transformer-XL, don't encode a token's absolute position but rather the relative distance between the query and key tokens in the self-attention calculation. This can be more effective for very long sequences, as it focuses on the local and long-distance relationships between words, which is often more semantically relevant.

* Rotary Positional Embeddings (RoPE): This method, popular in modern models like LLaMA and PaLM, modifies the Query and Key vectors by applying a rotation matrix that is dependent on the token's position. This allows the model to naturally encode relative positional information within the attention mechanism itself, without adding to the input embeddings.
